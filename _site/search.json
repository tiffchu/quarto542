[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "lab2_tchu2001",
    "section": "",
    "text": "A tutorial and explanation using unsupervised machine learning\nMembers of the United States Congress face long-standing concerns around conflicts of interest due to their access to market-moving information and their role in shaping economic policy. The STOCK Act requires lawmakers to publicly disclose their financial transactions, creating an opportunity for data-driven analysis of Congressional trading behavior.\nHowever, transparency alone does not guarantee accountability. With hundreds of lawmakers and tens of thousands of disclosed trades, identifying suspicious activity manually is impractical. This project explores how machine learning–based anomaly detection can help flag unusual trading patterns that deserve closer scrutiny without making accusations or legal claims.\nUsing only public data, this tutorial demonstrates how unsupervised models can reduce a complex investigative problem into a manageable set of high-risk cases for journalists, researchers, and oversight bodies to examine more closely.\n\n\nCongressional stock trading data is both large and structurally complex: * Hundreds of individual lawmakers * Thousands of trades across different industries * Varying trade sizes, timing, and market conditions * Sparse labels - we rarely know which trades are illegal\nThis makes the problem ill-suited for traditional supervised machine learning. We do not have ground-truth labels for “insider trading,” and even confirmed cases are extremely rare.\nYet the issue is far from hypothetical. Former Congressman Chris Collins used non-public information about a pharmaceutical trial to sell shares and avoid over $700,000 in losses. A New York Times analysis found that roughly one-fifth of Congress members traded stocks related to industries overseen by their committees.\nThese examples highlight the need for scalable, computational analysis. Machine learning can help by identifying trades that look unusual compared to typical behavior.\n\n\n\nThe goal of this project is not to accuse individuals of wrongdoing. Instead, it aims to automatically identify trades that are statistically unusual compared to typical Congressional trading behavior. You can follow along by using the code in this repository.\nThese flagged trades can then be examined manually with additional context, such as committee memberships, news events, or regulatory actions. Machine learning narrows the search space, then humans provide interpretation and judgment\n\n\n\nTop earning congress members in 2024 through trading\n\n\n\n\n\nThis project uses only publicly available data, including:\n\nCongressional trade disclosures\n\n\nSource: Senate stock transaction data\nFields include transaction date, ticker, trade type, and trade value\n\n\nHistorical stock price data: S&P Data from Yfinance\n\n\nUsed to calculate post-trade returns and market context\n\n\nContextual features, time series aggregates\n\n\nTime windows relative to trades\nAggregated return statistics The resulting dataset contains thousands of observations, each representing a single trade combined with numerical features showing the current market.\n\n\n\n\n\n\nBefore applying machine learning, the raw data must be cleaned and standardized.\nHandling Missing and Inconsistent Values Financial disclosure data from our dataset contains: * Ranges instead of exact trade values * Missing prices due to non-trading days * Inconsistent ticker formats These are addressed through: * Converting value ranges into numerical midpoints * Dropping trades with insufficient market data * Aligning trade dates with nearest available trading days This step is critical as anomaly detection is highly sensitive to noise.\n\n\n\nBecause anomaly detection algorithms rely on distance metrics, feature scaling is essential. All numerical features are standardized so that large-scale features (e.g., returns) do not dominate smaller-scale ones This ensures the model treats each signal proportionately.\n\n\n\nRaw trades alone are not informative enough. The model needs features that describe how unusual a trade is relative to normal behavior. Key engineered features include: * Post-trade returns over different time horizons * Relative performance compared to the broader market * Trade frequency patterns * Magnitude of price movement following the trade For example, a trade followed by unusually high short-term returns may be more suspicious than one aligned with normal market fluctuations. These features transform qualitative concerns (“like the trade looking lucky or coincidental”) into quantitative signals.\n\n\n\nUnlike fraud detection in credit cards, we do not have labeled examples of insider trading for Congress. So we inspect which trades look most different from the majority of Congressional trades? Unsupervised anomaly detection models are ideal for this setting because they learn what “normal” looks like and flags deviations without prior labels. This project explores two complementary approaches and combines results from both.\n\n\n\nAgglomerative clustering is a hierarchical method that groups similar data points together. The intuition behind this is that most trades behave similarly and cluster together and anomalous trades form small, distant clusters. The algorithm starts with each trade as its own cluster and iteratively merges the closest clusters until a predefined structure is reached. Trades belonging to very small or isolated clusters are treated as potential anomalies. Pros: Simple and interpretable, effective for identifying extreme outliers, and no assumptions about data distribution. This method serves as a baseline anomaly detector and provides intuition about the structure of the data.\n\n\n\n\n\n\nFigure 1: After dimensionality reduction of the dataset and using the clustering model resulting labels to define cluster colors\n\n\n\n\n\n\n\n\n\nFigure 2: dendrogram cluster visualization longer vertical lines connecting points = larger distances (the second cluster)\n\n\n\n\n\n\nTo capture more subtle patterns, the project also uses an autoencoder, a type of neural network designed for unsupervised learning. An autoencoder learns to: Compress the input data into a low-dimensional representation Reconstruct the original data from that representation During training, it learns the dominant patterns present in the dataset. Anomaly detection via reconstruction error: Normal trades are reconstructed accurately and unusual trades have high reconstruction error This error becomes the anomaly score. Pros: anomalies are subtle rather than extreme and relationships between features are non-linear\n\n\n\n\n\n\nFigure 3: Principal Component Analysis (PCA) on a dataset to reduce dimensionality to two component and then displays the transformed data in a scatter plot colored by cluster labels\n\n\n\n\n\n\n\n\n\nFigure 4: Anomaly-Detected Congress members and the top investors in that group\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nStrengths\nLimitations\n\n\n\n\nAgglomerative Clustering\nSimple, interpretable\nMisses subtle anomalies\n\n\nAutoencoder\nCaptures complex patterns\nLess transparent\n\n\n\nUsing both methods provides robustness: trades flagged by multiple models deserve particular attention.\n\n\nVisualization plays a crucial role in interpretation. Above, we used: * Dimensionality reduction plots (PCA) showing separation between normal and flagged trades * Ranked bar charts of the most suspicious trades and lawmakers These visuals help answer: How extreme are flagged trades compared to the norm? Are anomalies isolated or systematic?\n\n\n\n\nThe models consistently flag trades that: * Exhibit unusually high short-term returns * Occur at statistically rare times relative to market movements * Appear inconsistent with typical Congressional trading behavior Importantly, being flagged does not imply guilt. Innocent behavior can trigger anomalies, especially during volatile markets. However, the key advantage is scale reduction, from thousands of trades to a small subset worthy of deeper investigation"
  },
  {
    "objectID": "index.html#why-congressional-trading-is-a-data-science-problem",
    "href": "index.html#why-congressional-trading-is-a-data-science-problem",
    "title": "lab2_tchu2001",
    "section": "",
    "text": "Congressional stock trading data is both large and structurally complex: * Hundreds of individual lawmakers * Thousands of trades across different industries * Varying trade sizes, timing, and market conditions * Sparse labels - we rarely know which trades are illegal\nThis makes the problem ill-suited for traditional supervised machine learning. We do not have ground-truth labels for “insider trading,” and even confirmed cases are extremely rare.\nYet the issue is far from hypothetical. Former Congressman Chris Collins used non-public information about a pharmaceutical trial to sell shares and avoid over $700,000 in losses. A New York Times analysis found that roughly one-fifth of Congress members traded stocks related to industries overseen by their committees.\nThese examples highlight the need for scalable, computational analysis. Machine learning can help by identifying trades that look unusual compared to typical behavior."
  },
  {
    "objectID": "index.html#project-goal",
    "href": "index.html#project-goal",
    "title": "lab2_tchu2001",
    "section": "",
    "text": "The goal of this project is not to accuse individuals of wrongdoing. Instead, it aims to automatically identify trades that are statistically unusual compared to typical Congressional trading behavior. You can follow along by using the code in this repository.\nThese flagged trades can then be examined manually with additional context, such as committee memberships, news events, or regulatory actions. Machine learning narrows the search space, then humans provide interpretation and judgment\n\n\n\nTop earning congress members in 2024 through trading"
  },
  {
    "objectID": "index.html#data-sources-and-structure",
    "href": "index.html#data-sources-and-structure",
    "title": "lab2_tchu2001",
    "section": "",
    "text": "This project uses only publicly available data, including:\n\nCongressional trade disclosures\n\n\nSource: Senate stock transaction data\nFields include transaction date, ticker, trade type, and trade value\n\n\nHistorical stock price data: S&P Data from Yfinance\n\n\nUsed to calculate post-trade returns and market context\n\n\nContextual features, time series aggregates\n\n\nTime windows relative to trades\nAggregated return statistics The resulting dataset contains thousands of observations, each representing a single trade combined with numerical features showing the current market."
  },
  {
    "objectID": "index.html#tutorial",
    "href": "index.html#tutorial",
    "title": "lab2_tchu2001",
    "section": "",
    "text": "Before applying machine learning, the raw data must be cleaned and standardized.\nHandling Missing and Inconsistent Values Financial disclosure data from our dataset contains: * Ranges instead of exact trade values * Missing prices due to non-trading days * Inconsistent ticker formats These are addressed through: * Converting value ranges into numerical midpoints * Dropping trades with insufficient market data * Aligning trade dates with nearest available trading days This step is critical as anomaly detection is highly sensitive to noise.\n\n\n\nBecause anomaly detection algorithms rely on distance metrics, feature scaling is essential. All numerical features are standardized so that large-scale features (e.g., returns) do not dominate smaller-scale ones This ensures the model treats each signal proportionately.\n\n\n\nRaw trades alone are not informative enough. The model needs features that describe how unusual a trade is relative to normal behavior. Key engineered features include: * Post-trade returns over different time horizons * Relative performance compared to the broader market * Trade frequency patterns * Magnitude of price movement following the trade For example, a trade followed by unusually high short-term returns may be more suspicious than one aligned with normal market fluctuations. These features transform qualitative concerns (“like the trade looking lucky or coincidental”) into quantitative signals.\n\n\n\nUnlike fraud detection in credit cards, we do not have labeled examples of insider trading for Congress. So we inspect which trades look most different from the majority of Congressional trades? Unsupervised anomaly detection models are ideal for this setting because they learn what “normal” looks like and flags deviations without prior labels. This project explores two complementary approaches and combines results from both.\n\n\n\nAgglomerative clustering is a hierarchical method that groups similar data points together. The intuition behind this is that most trades behave similarly and cluster together and anomalous trades form small, distant clusters. The algorithm starts with each trade as its own cluster and iteratively merges the closest clusters until a predefined structure is reached. Trades belonging to very small or isolated clusters are treated as potential anomalies. Pros: Simple and interpretable, effective for identifying extreme outliers, and no assumptions about data distribution. This method serves as a baseline anomaly detector and provides intuition about the structure of the data.\n\n\n\n\n\n\nFigure 1: After dimensionality reduction of the dataset and using the clustering model resulting labels to define cluster colors\n\n\n\n\n\n\n\n\n\nFigure 2: dendrogram cluster visualization longer vertical lines connecting points = larger distances (the second cluster)\n\n\n\n\n\n\nTo capture more subtle patterns, the project also uses an autoencoder, a type of neural network designed for unsupervised learning. An autoencoder learns to: Compress the input data into a low-dimensional representation Reconstruct the original data from that representation During training, it learns the dominant patterns present in the dataset. Anomaly detection via reconstruction error: Normal trades are reconstructed accurately and unusual trades have high reconstruction error This error becomes the anomaly score. Pros: anomalies are subtle rather than extreme and relationships between features are non-linear\n\n\n\n\n\n\nFigure 3: Principal Component Analysis (PCA) on a dataset to reduce dimensionality to two component and then displays the transformed data in a scatter plot colored by cluster labels\n\n\n\n\n\n\n\n\n\nFigure 4: Anomaly-Detected Congress members and the top investors in that group\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nStrengths\nLimitations\n\n\n\n\nAgglomerative Clustering\nSimple, interpretable\nMisses subtle anomalies\n\n\nAutoencoder\nCaptures complex patterns\nLess transparent\n\n\n\nUsing both methods provides robustness: trades flagged by multiple models deserve particular attention.\n\n\nVisualization plays a crucial role in interpretation. Above, we used: * Dimensionality reduction plots (PCA) showing separation between normal and flagged trades * Ranked bar charts of the most suspicious trades and lawmakers These visuals help answer: How extreme are flagged trades compared to the norm? Are anomalies isolated or systematic?\n\n\n\n\nThe models consistently flag trades that: * Exhibit unusually high short-term returns * Occur at statistically rare times relative to market movements * Appear inconsistent with typical Congressional trading behavior Importantly, being flagged does not imply guilt. Innocent behavior can trigger anomalies, especially during volatile markets. However, the key advantage is scale reduction, from thousands of trades to a small subset worthy of deeper investigation"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]